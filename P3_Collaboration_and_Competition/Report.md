# Project 3. Collaboration and Competition

In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.

The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.

The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,

  - After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.
  - This yields a single score for each episode.

The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5.

## MADDPG Algorithm

I implmeneted MADDPG algorithm to solve this project.

![maddpg.jpg](https://i.loli.net/2019/12/09/isopfneyNGlV1Hq.jpg)

This paper *Multi-agent Actor-Critic for Mixed Cooperative-Competitive Environments* aims to solve the problem of multi-agent in mixed mode (cooperation-competition), and proposes MADDPG algorithm, which is essentially an extension and extension of DDPG algorithm. A very interesting viewpoint is proved in this paper:

As the number of agents increases, the decrease of the gradient in the right direction is exponentially related to the number of agents

The architecture of MADDPG is that each agent uses its own independent actor to output the determined action a by observing the state o by itself. Meanwhile, the training data only USES the training data generated by itself. Each agent also corresponds to a shadow, but this shadow receives data generated by all actors at the same time. The difference between a central shadow and a normal central shadow is that there are N of them.

![architecture.jpg](https://i.loli.net/2019/12/09/NUQYtukI48M7VFR.jpg)

The policy gradient of each agent can be described as:

![gradient.jpg](https://i.loli.net/2019/12/09/ImMghoUQxz6BH42.jpg)

## The Model Architectures and Hyperparameters Used
The Actor network is a three-layer neural network with 24 units in the input layer, and first hidden layer has 256 units, the second hidden layer has 128 units, and the final output layer has 2 units. The Critic network is constructed using a two-layer neural network similar to the number of units in the Actor network, except that there are 48 units in the input layer and 1 unit in the output layer. For faster learning, both networks use the elu activation function with Adam optimizer. For hyperparameters, replay buffer size is 1e5, minibatch size is 128, discount factor is 0.99, soft update of target parameters TAU is 1e-3, learning rate of the actor and critic is 1e-4 and 3e-4, and L2 weight decay is 0. Exploration Decay Rate begins from 1.0 with decay rate 0.999 to 0.1. In order to get the best strategy, I gradually reduced the probability of exploration. The mean value of noise processing is 0, theta is 0.15 and the sigma is 0.2.

## Result

![result.png](https://i.loli.net/2019/12/09/U7uDlE8Jj19NLdI.png)

The agents solved the enviroment and achieved +0.5 rewards in 1086 episodes.

## Future Improvements

Methods to further improve agent performance:
  - Different neural network architectures
  - Use Prioritized Experience Replay Buffer
  - Use Parameter Space Noise
